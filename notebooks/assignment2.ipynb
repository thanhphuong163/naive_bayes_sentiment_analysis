{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "cb5385c8",
=======
   "id": "999ff804",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "source": [
    "# Naive Bayes Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< HEAD
   "id": "8759b5ee",
=======
   "id": "acd1e0ba",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ntphuong/Dropbox/courses/cs579/notebooks/naive_bayes_sentiment_analysis/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "sys.path.append(cur_dir)\n",
    "print(cur_dir)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f3df9d98",
=======
   "id": "4668b058",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< HEAD
   "id": "6f4169b9",
=======
   "id": "dbf08a5d",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/amazon.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = sorted(glob.glob(\"../datasets/*.txt\"))\n",
    "data_dict = {\"sentence\": [], \"label\": []}\n",
    "print(data_paths[0])\n",
    "with open(data_paths[0], \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        s = line.strip().split(\"\\t\")\n",
    "        data_dict[\"sentence\"].append(s[0])\n",
    "        data_dict[\"label\"].append(int(s[1]))\n",
    "data_df = pd.DataFrame(data_dict)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "3f73d501",
=======
   "id": "4a3ed9e1",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "source": [
    "## Task 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
   "id": "92911155",
=======
   "id": "14debad6",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    my_stopwords = [w.strip() for w in f.readlines()]\n",
    "\n",
    "\n",
    "# Convert to lowercase\n",
    "def lowercase(sent):\n",
    "    return sent.lower()\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punctuation(sent):\n",
    "    return re.sub(\"[^0-9A-Za-z ]\", \"\", sent)\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(sent):\n",
    "    words = [w for w in word_tokenize(sent) if w not in my_stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Deal with negations\n",
    "def handle_negation(sent):\n",
    "    words = []\n",
    "    negation = \"\"\n",
    "    for w in word_tokenize(sent):\n",
    "        if re.search(r\"n[\\'o]t\", w):\n",
    "            negation = \"not_\"\n",
    "            continue\n",
    "        elif re.search(r\"\\W\", w):\n",
    "            negation = \"\"\n",
    "        w = negation + w\n",
    "        words.append(w)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Split training/test sets\n",
    "def split_data(sents, df, data_name):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(sents, df[\"label\"], test_size=0.2)\n",
    "    train = pd.DataFrame(data={\"sentence\": train_X, \"label\": train_y})\n",
    "    test = pd.DataFrame(data={\"sentence\": test_X, \"label\": test_y})\n",
    "    train.to_csv(f\"../preprocessed_datasets/{data_name}_train.txt\")\n",
    "    test.to_csv(f\"../preprocessed_datasets/{data_name}_test.txt\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# Build the bag of word representations\n",
    "def build_BoW(df, vectorizer=CountVectorizer()):\n",
    "    X = vectorizer.fit_transform(df[\"sentence\"]).toarray()\n",
    "    y = df[\"label\"].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< HEAD
   "id": "c20ed3e9",
=======
   "id": "50558dbf",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1690)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = [\n",
    "    lowercase,\n",
    "    handle_negation,\n",
    "    remove_stopwords,\n",
    "    remove_punctuation\n",
    "]\n",
    "new_sents = []\n",
    "sents = data_df[\"sentence\"]\n",
    "for sent in sents:\n",
    "    new_sent = sent\n",
    "    for transform in transforms:\n",
    "        new_sent = transform(new_sent)\n",
    "    new_sents.append(new_sent.strip())\n",
    "\n",
    "train, test = split_data(new_sents, data_df, \"amazon\")\n",
    "vectorizer = CountVectorizer()\n",
<<<<<<< HEAD
    "X, y = build_BoW(train, vectorizer)"
=======
    "X, y = build_BoW(train, vectorizer)\n",
    "X.shape"
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "a6e7f495",
=======
   "id": "d2b0d44b",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "source": [
    "## Task 2: Train Multinomial Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
<<<<<<< HEAD
   "id": "7a9c4270",
=======
   "id": "64aaef48",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1.0)  # The parameter alpha=1.0 is the add-1 laplace smoothing\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "4e333b9c",
=======
   "id": "23cecaa2",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "source": [
    "## Task 3: Train Binary Multinominal Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "eec9c432",
=======
   "id": "0009fd5c",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "source": [
    "## Task 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
<<<<<<< HEAD
   "id": "62a32dca",
=======
   "id": "f6ae8baa",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectorizer.transform(test[\"sentence\"])\n",
    "y_test = test[\"label\"].values\n",
    "y_hat = clf.predict(X_test)\n",
    "accuracy_score(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "16b10657",
=======
   "id": "959d0fa9",
>>>>>>> 7bfddc7d1a2ef1e9bc81429f04f7b3a3f81bc125
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
