{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0932ff13",
   "metadata": {},
   "source": [
    "# Naive Bayes Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7021d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ntphuong/Dropbox/courses/cs579/notebooks/naive_bayes_sentiment_analysis/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "sys.path.append(cur_dir)\n",
    "print(cur_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e929b1",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c04fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = sorted(glob.glob(\"../datasets/*.txt\"))\n",
    "data_dict = {\"sentence\": [], \"label\": []}\n",
    "data_path = data_paths[2]\n",
    "data_name = data_path.split(\"/\")[-1][:-4]\n",
    "print(data_name)\n",
    "with open(data_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        s = line.strip().split(\"\\t\")\n",
    "        data_dict[\"sentence\"].append(s[0])\n",
    "        data_dict[\"label\"].append(int(s[1]))\n",
    "data_df = pd.DataFrame(data_dict)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42870b8a",
   "metadata": {},
   "source": [
    "## Task 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a0d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    my_stopwords = [w.strip() for w in f.readlines()]\n",
    "\n",
    "\n",
    "# Convert to lowercase\n",
    "def lowercase(sent):\n",
    "    return sent.lower()\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punctuation(sent):\n",
    "    return re.sub(\"[^0-9A-Za-z ]\", \"\", sent)\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(sent):\n",
    "    words = [w for w in word_tokenize(sent) if w not in my_stopwords]\n",
    "#     words = [w for w in word_tokenize(sent) if w not in stopwords.words(\"english\")]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Deal with negations\n",
    "def handle_negation(sent):\n",
    "    words = []\n",
    "    negation = \"\"\n",
    "    for w in word_tokenize(sent):\n",
    "        if re.search(r\"n[\\'o]t\", w):\n",
    "            negation = \"not_\"\n",
    "            continue\n",
    "        elif re.search(r\"\\W\", w):\n",
    "            negation = \"\"\n",
    "        w = negation + w\n",
    "        words.append(w)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Split training/test sets\n",
    "def split_data(sents, df, data_name):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(sents, df[\"label\"], test_size=0.2)\n",
    "    train = pd.DataFrame(data={\"sentence\": train_X, \"label\": train_y})\n",
    "    test = pd.DataFrame(data={\"sentence\": test_X, \"label\": test_y})\n",
    "    train.to_csv(f\"../preprocessed_datasets/{data_name}_train.txt\")\n",
    "    test.to_csv(f\"../preprocessed_datasets/{data_name}_test.txt\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# Build the bag of word representations\n",
    "def build_BoW(df, vectorizer=CountVectorizer()):\n",
    "    X = vectorizer.fit_transform(df[\"sentence\"]).toarray()\n",
    "    y = df[\"label\"].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc150cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    lowercase,\n",
    "    handle_negation,\n",
    "    remove_stopwords,\n",
    "    remove_punctuation\n",
    "]\n",
    "new_sents = []\n",
    "sents = data_df[\"sentence\"]\n",
    "for sent in sents:\n",
    "    new_sent = sent\n",
    "    for transform in transforms:\n",
    "        new_sent = transform(new_sent)\n",
    "    new_sents.append(new_sent.strip())\n",
    "\n",
    "train, test = split_data(new_sents, data_df, data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd076177",
   "metadata": {},
   "source": [
    "## Task 2: Train Multinomial Na誰ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2058dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X, y = build_BoW(train, vectorizer)\n",
    "# The parameter alpha=1.0 is the add-1 laplace smoothing\n",
    "multinomial_clf = MultinomialNB(alpha=1.0)\n",
    "multinomial_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae942fa9",
   "metadata": {},
   "source": [
    "## Task 3: Train Binary Multinominal Na誰ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1ccbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameter binary=True is to clip all word counts each document at 1\n",
    "bin_vectorizer = CountVectorizer(binary=True)\n",
    "X_, y_ = build_BoW(train, bin_vectorizer)\n",
    "# The parameter alpha=1.0 is the add-1 laplace smoothing\n",
    "bin_multinomial_clf = MultinomialNB(alpha=1.0)\n",
    "bin_multinomial_clf.fit(X_, y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914370f",
   "metadata": {},
   "source": [
    "## Task 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf5b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.sum()=4346\n",
      "accuracy_score(y_hat, y_test)=0.82500\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Multinomial Na誰ve Bayes\n",
    "X_test = vectorizer.transform(test[\"sentence\"])\n",
    "y_test = test[\"label\"].values\n",
    "y_hat = multinomial_clf.predict(X_test)\n",
    "print(f\"{X.sum()=}\")\n",
    "print(f\"{accuracy_score(y_hat, y_test)=:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9a5c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_.sum()=4282\n",
      "accuracy_score(bin_y_hat, y_test)=0.82500\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Binary Multinomial Na誰ve Bayes\n",
    "bin_X_test = bin_vectorizer.transform(test[\"sentence\"])\n",
    "y_test = test[\"label\"].values\n",
    "bin_y_hat = bin_multinomial_clf.predict(bin_X_test)\n",
    "print(f\"{X_.sum()=}\")\n",
    "print(f\"{accuracy_score(bin_y_hat, y_test)=:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8cef50",
   "metadata": {},
   "source": [
    "Both Multinomial Naive Bayes and Binary Multinomial Naive Bayes have similar accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
